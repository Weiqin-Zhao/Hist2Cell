{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the seed for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "setup_seed(3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definet the device used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gpu_list = [2]\n",
    "gpu_list_str = ','.join(map(str, gpu_list))\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", gpu_list_str)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `Hist2Cell` model, and load the model on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch_geometric.nn import GATv2Conv, LayerNorm\n",
    "from model.ViT import Mlp, VisionTransformer\n",
    "\n",
    "\n",
    "class Hist2Cell(nn.Module):\n",
    "    def __init__(self, cell_dim=80, vit_depth=3):\n",
    "        super(Hist2Cell, self).__init__()\n",
    "        self.resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.resnet18 = torch.nn.Sequential(*list(self.resnet18.children())[:-1])\n",
    "        \n",
    "        self.embed_dim = 32 * 8\n",
    "        self.head = 8\n",
    "        self.dropout = 0.3\n",
    "        \n",
    "        self.conv1 = GATv2Conv(in_channels=512, out_channels=int(self.embed_dim/self.head), heads=self.head)\n",
    "        self.norm1 = LayerNorm(in_channels=self.embed_dim)\n",
    "        \n",
    "        self.cell_transformer = VisionTransformer(num_classes=cell_dim, embed_dim=self.embed_dim, depth=vit_depth,\n",
    "                                                  mlp_head=True, drop_rate=self.dropout, attn_drop_rate=self.dropout)\n",
    "        self.spot_fc = Linear(in_features=512, out_features=256)\n",
    "        self.spot_head = Mlp(in_features=256, hidden_features=512*2, out_features=cell_dim)\n",
    "        self.local_head = Mlp(in_features=256, hidden_features=512*2, out_features=cell_dim)\n",
    "        self.fused_head = Mlp(in_features=256, hidden_features=512*2, out_features=cell_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x_spot = self.resnet18(x)\n",
    "        x_spot = x_spot.squeeze()\n",
    "        \n",
    "        x_local = self.conv1(x=x_spot, edge_index=edge_index)\n",
    "        x_local = self.norm1(x_local)\n",
    "        \n",
    "        x_local = x_local.unsqueeze(0)\n",
    "        \n",
    "        x_cell = x_local\n",
    "        \n",
    "        x_spot = self.spot_fc(x_spot)\n",
    "        cell_predication_spot = self.spot_head(x_spot)\n",
    "        x_local = x_local.squeeze(0)\n",
    "        cell_prediction_local = self.local_head(x_local)\n",
    "        cell_prediction_global, x_global = self.cell_transformer(x_cell)\n",
    "        cell_prediction_global = cell_prediction_global.squeeze()\n",
    "        x_global = x_global.squeeze()\n",
    "        cell_prediction_fused = self.fused_head((x_spot+x_local+x_global)/3.0)\n",
    "        cell_prediction = (cell_predication_spot + cell_prediction_local + cell_prediction_global + cell_prediction_fused) / 4.0\n",
    "        \n",
    "        return cell_prediction\n",
    "    \n",
    "    \n",
    "model = Hist2Cell(vit_depth=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train/test split file, here we train `Hist2Cell` on other 3 donors in the humanlung cell2location dataset, and test `Hist2Cell` on the left donnor A50:\n",
    "\n",
    "There are 2 slides from donor A50 in humanlung cell2location dataset: \n",
    "- WSA_LngSP9258463\n",
    "- WSA_LngSP9258467\n",
    "\n",
    "The slides from the other 3 donors used for training are:\n",
    "- WSA_LngSP8759311\n",
    "- WSA_LngSP8759312\n",
    "- WSA_LngSP8759313\n",
    "- WSA_LngSP9258464\n",
    "- WSA_LngSP9258468\n",
    "- WSA_LngSP10193347\n",
    "- WSA_LngSP10193348\n",
    "- WSA_LngSP10193345\n",
    "- WSA_LngSP10193346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slides = open(\"./train_test_splits/humanlung_cell2location/train_leave_A50.txt\").read().split('\\n')\n",
    "test_slides = open(\"./train_test_splits/humanlung_cell2location/test_leave_A50.txt\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the processed data for each slide as the train/test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "train_graph_list = list()\n",
    "for item in train_slides:\n",
    "    train_graph_list.append(torch.load(os.path.join(\"./example_data/humanlung_cell2location\", item+'.pt')))\n",
    "train_dataset = Batch.from_data_list(train_graph_list)\n",
    "\n",
    "test_graph_list = list()\n",
    "for item in test_slides:\n",
    "    test_graph_list.append(torch.load(os.path.join(\"./example_data/humanlung_cell2location\", item+'.pt')))\n",
    "test_dataset = Batch.from_data_list(test_graph_list)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `DataLoader` for train/test dataset, here are 2 important parameters:\n",
    "- `hop`: this parameter define receptive field when sampling the subgraphs with a group of center nodes for training/testing, in our paper, we use 2-hop subgraphs to achieve a banlance between computation cost and performance, generally, bigger receptive field will contain more neighboring information.\n",
    "- `subgraph_bs`: this parameter define the number of subgraphs to be sampled during training/testing, which is the `subgraph batchsize`, we use `subgraph_bs=16` on our RTX 3090 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "import torch_geometric\n",
    "torch_geometric.typing.WITH_PYG_LIB = False\n",
    "\n",
    "\n",
    "hop = 2\n",
    "subgraph_bs = 16\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    train_dataset,\n",
    "    num_neighbors=[-1]*hop,\n",
    "    batch_size=subgraph_bs,\n",
    "    directed=False,\n",
    "    input_nodes=None,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    test_dataset,\n",
    "    num_neighbors=[-1]*hop,\n",
    "    batch_size=subgraph_bs,\n",
    "    directed=False,\n",
    "    input_nodes=None,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `learning rate`, `criterion`, `optimizer` and `scheduler` used for training, we use `lr=1e-4` in our study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "\n",
    "params = model.parameters()\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-5, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple example, we train `Hist2Cell` for 5 epochs, and save the checkpoint with the best Pearson R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch: 1 \t\n",
      "lr =  0.0001\n",
      "saving best cell all abundance average 0.23149620760288295\n",
      "Training complete in 7m 26s\n",
      "Epoch: 1 \tTraining Cell abundance Loss: 0.254149\n",
      "Epoch: 1 \tTraining Cell abundance pearson all average: 0.675193\n",
      "Epoch: 1 \tTest Cell abundance Loss: 0.427099\n",
      "Epoch: 1 \tTest Cell abundance pearson all average: 0.231496\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch: 2 \t\n",
      "lr =  9.94459753267812e-05\n",
      "saving best cell all abundance average 0.2527291595356535\n",
      "Training complete in 12m 47s\n",
      "Epoch: 2 \tTraining Cell abundance Loss: 0.083678\n",
      "Epoch: 2 \tTraining Cell abundance pearson all average: 0.806925\n",
      "Epoch: 2 \tTest Cell abundance Loss: 0.473617\n",
      "Epoch: 2 \tTest Cell abundance pearson all average: 0.252729\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch: 3 \t\n",
      "lr =  9.779754323328192e-05\n",
      "saving best cell all abundance average 0.25982980173108405\n",
      "Training complete in 18m 12s\n",
      "Epoch: 3 \tTraining Cell abundance Loss: 0.055555\n",
      "Epoch: 3 \tTraining Cell abundance pearson all average: 0.839837\n",
      "Epoch: 3 \tTest Cell abundance Loss: 0.477435\n",
      "Epoch: 3 \tTest Cell abundance pearson all average: 0.259830\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch: 4 \t\n",
      "lr =  9.509529358847657e-05\n",
      "saving best cell all abundance average 0.27311999407604454\n",
      "Training complete in 23m 35s\n",
      "Epoch: 4 \tTraining Cell abundance Loss: 0.043770\n",
      "Epoch: 4 \tTraining Cell abundance pearson all average: 0.861942\n",
      "Epoch: 4 \tTest Cell abundance Loss: 0.490737\n",
      "Epoch: 4 \tTest Cell abundance pearson all average: 0.273120\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch: 5 \t\n",
      "lr =  9.140576474687265e-05\n",
      "Training complete in 28m 60s\n",
      "Epoch: 5 \tTraining Cell abundance Loss: 0.035659\n",
      "Epoch: 5 \tTraining Cell abundance pearson all average: 0.880666\n",
      "Epoch: 5 \tTest Cell abundance Loss: 0.510857\n",
      "Epoch: 5 \tTest Cell abundance pearson all average: 0.257695\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "best_cell_abundance_all_average = 0.0\n",
    "since = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(\"---------------------------------------\"*4)\n",
    "    print('Epoch: {} \\t'.format(epoch + 1))\n",
    "    print('lr = ',optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    train_sample_num = 0\n",
    "    train_cell_pred_array = []\n",
    "    train_cell_label_array = []\n",
    "    train_cell_abundance_loss = 0\n",
    "    for graph in train_loader:\n",
    "        x = graph.x.to(device)\n",
    "        y = graph.y.to(device)\n",
    "        edge_index = graph.edge_index.to(device)\n",
    "        cell_label = y[:, 250:]\n",
    "        \n",
    "        cell_pred = model(x=x, edge_index=edge_index)\n",
    "\n",
    "        cell_loss = criterion(cell_pred, cell_label)        \n",
    "        optimizer.zero_grad()\n",
    "        cell_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        center_num = len(graph.input_id)\n",
    "        center_cell_label = cell_label[:center_num, :]\n",
    "        center_cell_pred = cell_pred[:center_num, :]\n",
    "        train_cell_label_array.append(center_cell_label.squeeze().cpu().detach().numpy())\n",
    "        train_cell_pred_array.append(center_cell_pred.squeeze().cpu().detach().numpy())\n",
    "        train_sample_num = train_sample_num + center_num\n",
    "        train_cell_abundance_loss += cell_loss.item() * center_num\n",
    "\n",
    "    train_cell_abundance_loss = train_cell_abundance_loss / train_sample_num\n",
    "    \n",
    "    if len(train_cell_pred_array[-1].shape) == 1:\n",
    "        train_cell_pred_array[-1] = np.expand_dims(train_cell_pred_array[-1], axis=0)\n",
    "    train_cell_pred_array = np.concatenate(train_cell_pred_array)\n",
    "    if len(train_cell_label_array[-1].shape) == 1:\n",
    "        train_cell_label_array[-1] = np.expand_dims(train_cell_label_array[-1], axis=0)\n",
    "    train_cell_label_array = np.concatenate(train_cell_label_array)\n",
    "\n",
    "    train_cell_abundance_all_pearson_average = 0.0\n",
    "    for i in range(train_cell_pred_array.shape[1]):\n",
    "        r, p = pearsonr(train_cell_pred_array[:, i], train_cell_label_array[:, i])\n",
    "        train_cell_abundance_all_pearson_average = train_cell_abundance_all_pearson_average + r\n",
    "    train_cell_abundance_all_pearson_average = train_cell_abundance_all_pearson_average / train_cell_pred_array.shape[1]\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        test_sample_num = 0\n",
    "        test_cell_pred_array = []\n",
    "        test_cell_label_array = []\n",
    "        test_cell_abundance_loss = 0\n",
    "        for graph in test_loader:\n",
    "            x = graph.x.to(device)\n",
    "            y = graph.y.to(device)\n",
    "            edge_index = graph.edge_index.to(device)\n",
    "            cell_label = y[:, 250:]\n",
    "            \n",
    "            cell_pred = model(x=x, edge_index=edge_index)\n",
    "\n",
    "            cell_loss = criterion(cell_pred, cell_label)\n",
    "\n",
    "            center_num = len(graph.input_id)\n",
    "            center_cell_label = cell_label[:center_num, :]\n",
    "            center_cell_pred = cell_pred[:center_num, :]\n",
    "            \n",
    "            test_cell_label_array.append(center_cell_label.squeeze().cpu().detach().numpy())\n",
    "            test_cell_pred_array.append(center_cell_pred.squeeze().cpu().detach().numpy())\n",
    "            test_sample_num = test_sample_num + center_num\n",
    "            \n",
    "            test_cell_abundance_loss += cell_loss.item() * center_num\n",
    "            \n",
    "        test_cell_abundance_loss = test_cell_abundance_loss / test_sample_num\n",
    " \n",
    "    if len(test_cell_pred_array[-1].shape) == 1:\n",
    "        test_cell_pred_array[-1] = np.expand_dims(test_cell_pred_array[-1], axis=0)\n",
    "    test_cell_pred_array = np.concatenate(test_cell_pred_array)\n",
    "    if len(test_cell_label_array[-1].shape) == 1:\n",
    "        test_cell_label_array[-1] = np.expand_dims(test_cell_label_array[-1], axis=0)\n",
    "    test_cell_label_array = np.concatenate(test_cell_label_array)\n",
    "        \n",
    "    test_cell_abundance_all_pearson_average = 0.0\n",
    "    for i in range(test_cell_pred_array.shape[1]):\n",
    "        r, p = pearsonr(test_cell_pred_array[:, i], test_cell_label_array[:, i])\n",
    "        test_cell_abundance_all_pearson_average = test_cell_abundance_all_pearson_average + r\n",
    "    test_cell_abundance_all_pearson_average = test_cell_abundance_all_pearson_average / test_cell_pred_array.shape[1]\n",
    "\n",
    "    if test_cell_abundance_all_pearson_average > best_cell_abundance_all_average:\n",
    "        best_cell_abundance_all_average = test_cell_abundance_all_pearson_average\n",
    "        torch.save(model.state_dict(), os.path.join(\"./model_weights\", \"demo_ckpt.pth\"))\n",
    "        print(\"saving \" + \"best cell all abundance average \" + str(test_cell_abundance_all_pearson_average))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
    "    print(f'Epoch: {(epoch + 1)} \\tTraining Cell abundance Loss: {train_cell_abundance_loss:.6f}')\n",
    "    print(f'Epoch: {(epoch + 1)} \\tTraining Cell abundance pearson all average: {train_cell_abundance_all_pearson_average:.6f}')\n",
    "    print(f'Epoch: {(epoch + 1)} \\tTest Cell abundance Loss: {test_cell_abundance_loss:.6f}')\n",
    "    print(f'Epoch: {(epoch + 1)} \\tTest Cell abundance pearson all average: {test_cell_abundance_all_pearson_average:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
