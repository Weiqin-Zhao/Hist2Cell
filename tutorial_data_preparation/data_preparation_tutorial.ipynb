{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist2Cell Data Preparation Tutorial\n",
    "\n",
    "## Welcome to the Hist2Cell Data Preparation Guide! üß¨\n",
    "\n",
    "This comprehensive tutorial will guide you through the essential process of preparing your spatial transcriptomics data for **Hist2Cell** training and inference. Whether you're new to spatial biology or an experienced researcher, this guide provides step-by-step instructions to transform your raw histology data into the format required by our Vision Graph-Transformer framework.\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [**Understanding Spatial Transcriptomics Data**](#understanding-spatial-transcriptomics-data)\n",
    "2. [**Hist2Cell Data Structure Overview**](#hist2cell-data-structure-overview)\n",
    "3. [**Exploring Processed Data Examples**](#exploring-processed-data-examples)\n",
    "4. [**Raw Data Structure Explanation**](#raw-data-structure-explanation)\n",
    "5. [**Step-by-Step Data Processing Pipeline**](#step-by-step-data-processing-pipeline)\n",
    "6. [**Graph Construction and Validation**](#graph-construction-and-validation)\n",
    "7. [**Troubleshooting and Best Practices**](#troubleshooting-and-best-practices)\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ‚úÖ Understand the spatial transcriptomics data structure used in Hist2Cell\n",
    "- ‚úÖ Learn how to process raw histology images and spatial gene expression data\n",
    "- ‚úÖ Master the graph construction process for spatial neighborhood relationships\n",
    "- ‚úÖ Prepare your own datasets for Hist2Cell training and inference\n",
    "- ‚úÖ Validate data quality and troubleshoot common issues\n",
    "\n",
    "## üî¨ Understanding Spatial Transcriptomics Data\n",
    "\n",
    "**Spatial Transcriptomics (ST)** is a cutting-edge technology that measures gene expression while preserving the spatial context of where each measurement was taken in tissue samples. Unlike traditional RNA-seq, which loses spatial information, ST provides:\n",
    "\n",
    "- **Spatial Gene Expression**: Gene expression profiles at specific tissue locations\n",
    "- **Histology Context**: High-resolution tissue images showing cellular architecture\n",
    "- **Spatial Relationships**: Information about how different tissue regions interact\n",
    "\n",
    "### Key Concepts:\n",
    "- **Spots**: Discrete locations where gene expression is measured (think of them as pixels in a gene expression image)\n",
    "- **Patches**: Image regions extracted around each spot from the histology slide\n",
    "- **Cell Deconvolution**: The process of estimating cell type abundances from bulk measurements\n",
    "- **Spatial Graphs**: Networks representing spatial relationships between neighboring spots\n",
    "\n",
    "## üèóÔ∏è Hist2Cell Architecture Overview\n",
    "\n",
    "**Hist2Cell** is a Vision Graph-Transformer framework that predicts fine-grained cell type abundances directly from histology images. The model combines:\n",
    "\n",
    "1. **Local Feature Extractor**: Analyzes histology image patches using ResNet18\n",
    "2. **Graph Neural Networks**: Captures spatial relationships between neighboring tissue regions\n",
    "3. **Transformer Architecture**: Processes global tissue context and patterns\n",
    "4. **Multi-scale Fusion**: Integrates information from spot, local, and global levels\n",
    "\n",
    "## üìä Data Requirements\n",
    "\n",
    "To prepare data for Hist2Cell, you need:\n",
    "\n",
    "### Required Files:\n",
    "- **Histology Images**: High-resolution tissue slide images (WSI format)\n",
    "- **Image Patches**: 224√ó224 pixel patches extracted around each spot\n",
    "- **Gene Expression Data**: Spatial gene expression measurements\n",
    "- **Cell Type Abundances**: Fine-grained cell type composition (from deconvolution)\n",
    "- **Spatial Coordinates**: X,Y positions of each spot on the slide\n",
    "\n",
    "### Provided Example Data:\n",
    "- `./example_data/humanlung_cell2location`: Processed data from healthy human lung tissue\n",
    "- `./example_data/humanlung_cell2location_2x`: Super-resolution data (2x higher resolution)\n",
    "\n",
    "Let's start by exploring the data structure and then learn how to process your own data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# üîç Exploring Processed Data Examples\n",
    "\n",
    "## Understanding the Hist2Cell Data Structure\n",
    "\n",
    "Before diving into data processing, let's examine the structure of processed data that Hist2Cell expects. We'll use slide `WSA_LngSP9258467` from donor A50 in our healthy human lung dataset as an example.\n",
    "\n",
    "### üìÅ Data Format: PyTorch Geometric\n",
    "\n",
    "Hist2Cell uses **PyTorch Geometric** format, which is specifically designed for graph neural networks. This format efficiently stores:\n",
    "- **Node features** (image patches and coordinates)\n",
    "- **Edge connections** (spatial relationships between neighboring spots)\n",
    "- **Labels** (gene expression and cell type abundances)\n",
    "\n",
    "### Loading a Processed Example\n",
    "\n",
    "Let's load a processed data file and explore its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "processed_data = torch.load(\"../example_data/humanlung_cell2location/WSA_LngSP9258467.pt\")\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Structure Breakdown\n",
    "\n",
    "The processed data contains the following key components:\n",
    "\n",
    "### üñºÔ∏è **Node Features (`x`)**: Image Patches\n",
    "- **Shape**: `[422, 3, 224, 224]` \n",
    "- **Meaning**: 224√ó224 RGB image patches for each of the 422 spots in this slide\n",
    "- **Purpose**: Each spot becomes a node in our spatial graph, and the image patch provides visual context\n",
    "- **Format**: Standard PyTorch tensor format (C√óH√óW) with values normalized for ResNet18\n",
    "\n",
    "### üîó **Graph Connectivity (`edge_index`)**: Spatial Relationships  \n",
    "- **Shape**: `[2, 2732]` (COO format)\n",
    "- **Meaning**: 2,732 edges connecting neighboring spots based on spatial proximity\n",
    "- **Purpose**: Defines which spots are neighbors in the tissue, enabling spatial information flow\n",
    "- **Format**: Each column represents an edge (source_node, target_node)\n",
    "\n",
    "### üè∑Ô∏è **Labels (`y`)**: Multi-modal Ground Truth\n",
    "- **Shape**: `[422, 330]` \n",
    "- **Composition**: \n",
    "  - **250 genes**: Top highly expressed genes (normalized expression values)\n",
    "  - **80 cell types**: Fine-grained cell type abundances (from deconvolution)\n",
    "- **Purpose**: Training targets for both gene expression and cell type prediction\n",
    "- **Total**: 330 labels per spot (250 + 80)\n",
    "\n",
    "### üìç **Spatial Coordinates (`pos`)**: Physical Positions\n",
    "- **Shape**: `[422, 2]`\n",
    "- **Meaning**: X,Y pixel coordinates of each spot on the original histology slide\n",
    "- **Purpose**: Used for visualization, spatial analysis, and cell co-localization metrics\n",
    "- **Units**: Pixel coordinates on the original whole slide image (WSI)\n",
    "\n",
    "### üí° Key Insights:\n",
    "- **Graph Structure**: This slide contains 422 spots (nodes) connected by 2,732 edges\n",
    "- **Spatial Density**: Each spot has ~6.5 neighbors on average (2,732 √ó 2 √∑ 422)\n",
    "- **Multi-scale Information**: Combines local image features with global spatial context\n",
    "- **Rich Labels**: Both gene expression and cell type information for comprehensive training\n",
    "\n",
    "Let's examine each component in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the image patch dimensions for the first spot\n",
    "print(\"üìä Image Patch Analysis:\")\n",
    "print(f\"   Individual patch shape: {processed_data['x'][0].shape}\")\n",
    "print(f\"   Format: [Channels, Height, Width]\")\n",
    "print(f\"   Channels: {processed_data['x'][0].shape[0]} (RGB)\")\n",
    "print(f\"   Image size: {processed_data['x'][0].shape[1]}√ó{processed_data['x'][0].shape[2]} pixels\")\n",
    "print(f\"   Total patches: {processed_data['x'].shape[0]}\")\n",
    "print(\"\\nüí° Each spot has a 224√ó224 RGB image patch extracted from the original slide\")\n",
    "print(\"   This provides visual context for the Hist2Cell model to analyze tissue architecture\")\n",
    "\n",
    "# Show the actual shape for reference\n",
    "processed_data['x'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the label structure for the first spot\n",
    "print(\"üè∑Ô∏è Label Analysis (First Spot):\")\n",
    "print(f\"   Total labels per spot: {processed_data['y'].shape[1]}\")\n",
    "print(f\"   Label composition: 250 genes + 80 cell types = 330 total\")\n",
    "print(f\"   First 5 gene expression values: {processed_data['y'][0][:5].tolist()}\")\n",
    "print(f\"   Sample cell type abundances (positions 250-255): {processed_data['y'][0][250:255].tolist()}\")\n",
    "print(\"\\nüí° Label Structure:\")\n",
    "print(\"   - Positions 0-249: Top 250 highly expressed genes (log-normalized)\")\n",
    "print(\"   - Positions 250-329: 80 fine-grained cell type abundances\")\n",
    "print(\"   - Values represent normalized expression levels or abundance fractions\")\n",
    "\n",
    "# Show the actual first 5 values for reference\n",
    "processed_data['y'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the graph connectivity structure\n",
    "print(\"üîó Graph Connectivity Analysis:\")\n",
    "print(f\"   Edge index shape: {processed_data['edge_index'].shape}\")\n",
    "print(f\"   Format: [2, num_edges] - COO (Coordinate) format\")\n",
    "print(f\"   Total edges: {processed_data['edge_index'].shape[1]}\")\n",
    "print(f\"   Source nodes (row 0): {processed_data['edge_index'][0][:10].tolist()}... (showing first 10)\")\n",
    "print(f\"   Target nodes (row 1): {processed_data['edge_index'][1][:10].tolist()}... (showing first 10)\")\n",
    "\n",
    "# Calculate connectivity statistics\n",
    "source_nodes = processed_data['edge_index'][0]\n",
    "target_nodes = processed_data['edge_index'][1]\n",
    "unique_nodes = torch.unique(torch.cat([source_nodes, target_nodes]))\n",
    "avg_degree = processed_data['edge_index'].shape[1] / processed_data['x'].shape[0]\n",
    "\n",
    "print(f\"\\nüìä Connectivity Statistics:\")\n",
    "print(f\"   Total nodes: {processed_data['x'].shape[0]}\")\n",
    "print(f\"   Connected nodes: {len(unique_nodes)}\")\n",
    "print(f\"   Average degree: {avg_degree:.2f} neighbors per spot\")\n",
    "print(f\"   Graph density: {(processed_data['edge_index'].shape[1] / (processed_data['x'].shape[0] * (processed_data['x'].shape[0] - 1))) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nüí° The graph represents spatial relationships between neighboring spots\")\n",
    "print(\"   Each edge connects two spatially adjacent spots in the tissue\")\n",
    "\n",
    "# Show the actual edge_index for reference\n",
    "processed_data['edge_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the spatial coordinate information\n",
    "print(\"üìç Spatial Coordinates Analysis:\")\n",
    "print(f\"   Position shape: {processed_data['pos'].shape}\")\n",
    "print(f\"   Format: [num_spots, 2] - (X, Y) pixel coordinates\")\n",
    "print(f\"   Sample coordinates (first 5 spots):\")\n",
    "for i in range(5):\n",
    "    x, y = processed_data['pos'][i]\n",
    "    print(f\"     Spot {i}: ({x:.0f}, {y:.0f}) pixels\")\n",
    "\n",
    "# Calculate spatial extent\n",
    "min_x, max_x = processed_data['pos'][:, 0].min(), processed_data['pos'][:, 0].max()\n",
    "min_y, max_y = processed_data['pos'][:, 1].min(), processed_data['pos'][:, 1].max()\n",
    "width = max_x - min_x\n",
    "height = max_y - min_y\n",
    "\n",
    "print(f\"\\nüìè Spatial Extent:\")\n",
    "print(f\"   X range: {min_x:.0f} to {max_x:.0f} pixels (width: {width:.0f})\")\n",
    "print(f\"   Y range: {min_y:.0f} to {max_y:.0f} pixels (height: {height:.0f})\")\n",
    "print(f\"   Tissue area: {width:.0f} √ó {height:.0f} pixels\")\n",
    "\n",
    "# Calculate average spot spacing\n",
    "import numpy as np\n",
    "positions = processed_data['pos'].numpy()\n",
    "distances = []\n",
    "for i in range(min(100, len(positions))):  # Sample 100 spots for efficiency\n",
    "    pos_i = positions[i]\n",
    "    other_pos = positions[np.arange(len(positions)) != i]\n",
    "    dists = np.sqrt(np.sum((other_pos - pos_i)**2, axis=1))\n",
    "    distances.append(np.min(dists))\n",
    "avg_spacing = np.mean(distances)\n",
    "\n",
    "print(f\"   Average spot spacing: {avg_spacing:.0f} pixels\")\n",
    "\n",
    "print(\"\\nüí° Spatial coordinates are used for:\")\n",
    "print(\"   - Visualizing predictions on tissue slides\")\n",
    "print(\"   - Calculating cell co-localization metrics\")\n",
    "print(\"   - Constructing spatial neighborhood graphs\")\n",
    "\n",
    "# Show the actual first 5 positions for reference\n",
    "processed_data['pos'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Subgraph Sampling with NeighborLoader\n",
    "\n",
    "For efficient training on large graphs, we use **subgraph sampling** via `NeighborLoader` from PyTorch Geometric. This technique is crucial for handling large spatial transcriptomics datasets that may contain thousands of spots.\n",
    "\n",
    "### üéØ Why Subgraph Sampling?\n",
    "\n",
    "**Challenge**: Processing entire graphs simultaneously requires enormous GPU memory and computational resources.\n",
    "\n",
    "**Solution**: Sample smaller subgraphs around center nodes for training/testing, maintaining spatial context while reducing memory usage.\n",
    "\n",
    "### üîß Key Parameters Explained\n",
    "\n",
    "#### 1. **`hop`**: Receptive Field Size\n",
    "- **Definition**: Number of neighborhood layers to include around center nodes\n",
    "- **Our choice**: `hop=2` (2-hop subgraphs)\n",
    "- **Meaning**: Include immediate neighbors (1-hop) + neighbors of neighbors (2-hop)\n",
    "- **Trade-off**: \n",
    "  - ‚úÖ Larger hop ‚Üí More spatial context, better performance\n",
    "  - ‚ùå Larger hop ‚Üí Higher memory usage, slower training\n",
    "\n",
    "#### 2. **`subgraph_bs`**: Subgraph Batch Size\n",
    "- **Definition**: Number of center nodes (subgraphs) processed simultaneously\n",
    "- **Our choice**: `subgraph_bs=16` (optimized for RTX 3090 24GB)\n",
    "- **GPU Memory Guidelines**:\n",
    "  - üü¢ RTX 3090 (24GB): `subgraph_bs=16`\n",
    "  - üü° RTX 3080 (12GB): `subgraph_bs=8`\n",
    "  - üî¥ RTX 3070 (8GB): `subgraph_bs=4`\n",
    "\n",
    "#### 3. **`num_neighbors`**: Sampling Strategy\n",
    "- **Our choice**: `[-1] * hop` (sample all neighbors)\n",
    "- **Alternative**: `[20, 10]` (sample 20 at 1-hop, 10 at 2-hop)\n",
    "- **Use case**: Limit neighbors for very dense graphs\n",
    "\n",
    "### üí° Benefits of This Approach:\n",
    "- **Memory Efficiency**: Process large graphs on consumer GPUs\n",
    "- **Spatial Context**: Maintain neighborhood relationships\n",
    "- **Scalability**: Handle datasets with thousands of spots\n",
    "- **Parallelization**: Batch multiple subgraphs for efficiency\n",
    "\n",
    "Let's configure and test the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for subgraph sampling\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import torch_geometric\n",
    "torch_geometric.typing.WITH_PYG_LIB = False\n",
    "\n",
    "# Configure subgraph sampling parameters\n",
    "hop = 2               # 2-hop neighborhood (optimal balance of context vs. memory)\n",
    "subgraph_bs = 16      # Batch size for RTX 3090 (adjust based on your GPU memory)\n",
    "\n",
    "print(\"‚öôÔ∏è DataLoader Configuration:\")\n",
    "print(f\"   Hop size: {hop} (include {hop}-hop neighbors)\")\n",
    "print(f\"   Subgraph batch size: {subgraph_bs}\")\n",
    "print(f\"   Total spots in slide: {processed_data.num_nodes}\")\n",
    "print(f\"   Expected batches per epoch: {processed_data.num_nodes // subgraph_bs}\")\n",
    "\n",
    "# Create the NeighborLoader for subgraph sampling\n",
    "dataloader_loader = NeighborLoader(\n",
    "    processed_data,\n",
    "    num_neighbors=[-1]*hop,    # Sample all neighbors at each hop\n",
    "    batch_size=subgraph_bs,    # Number of center nodes per batch\n",
    "    directed=False,            # Use undirected graph (spatial relationships are symmetric)\n",
    "    input_nodes=None,          # Use all nodes as potential center nodes\n",
    "    shuffle=True,              # Randomly sample center nodes for training\n",
    "    num_workers=2,             # Parallel data loading workers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ NeighborLoader created successfully!\")\n",
    "print(f\"   This will create subgraphs with {hop}-hop neighborhoods\")\n",
    "print(f\"   Each batch contains {subgraph_bs} center nodes and their neighbors\")\n",
    "print(f\"   The loader will iterate through all {processed_data.num_nodes} spots as center nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Subgraph Batching and Merging\n",
    "\n",
    "### How PyTorch Geometric Handles Subgraphs\n",
    "\n",
    "In PyTorch Geometric, multiple sampled subgraphs are **merged into a single large graph** for efficient parallel processing. This approach:\n",
    "\n",
    "1. **Batches Multiple Subgraphs**: Combines `subgraph_bs` subgraphs into one batch\n",
    "2. **Maintains Separation**: Each subgraph remains disconnected from others in the batch\n",
    "3. **Parallel Processing**: GPU can process all subgraphs simultaneously\n",
    "4. **Memory Efficiency**: Reduces overhead compared to processing individual subgraphs\n",
    "\n",
    "### üîç Key Features of Merged Subgraphs:\n",
    "\n",
    "- **Node Remapping**: Original node IDs are preserved in `n_id` field\n",
    "- **Edge Remapping**: Original edge IDs are preserved in `e_id` field  \n",
    "- **Input Identification**: `input_id` tracks which nodes were center nodes\n",
    "- **Batch Information**: `batch_size` indicates how many subgraphs are merged\n",
    "\n",
    "### üìä Expected Structure:\n",
    "- **Nodes**: Total nodes from all subgraphs (varies based on neighborhood size)\n",
    "- **Edges**: Total edges from all subgraphs (includes inter-subgraph disconnections)\n",
    "- **Features**: Same format as original data but for merged subgraphs\n",
    "\n",
    "Let's examine a sampled subgraph batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and analyze a subgraph batch\n",
    "print(\"üîç Analyzing Subgraph Sampling:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for subgraphs in dataloader_loader:\n",
    "    print(f\"üìä Subgraph Batch Structure:\")\n",
    "    print(f\"   Merged subgraph: {subgraphs}\")\n",
    "    print(f\"   Total nodes in batch: {subgraphs.num_nodes}\")\n",
    "    print(f\"   Total edges in batch: {subgraphs.num_edges}\")\n",
    "    print(f\"   Input nodes (centers): {subgraphs.input_id.shape[0]}\")\n",
    "    print(f\"   Actual batch size: {subgraphs.batch_size}\")\n",
    "    \n",
    "    # Analyze subgraph composition\n",
    "    print(f\"\\nüîç Subgraph Composition Analysis:\")\n",
    "    print(f\"   Average nodes per subgraph: {subgraphs.num_nodes / subgraphs.batch_size:.1f}\")\n",
    "    print(f\"   Average edges per subgraph: {subgraphs.num_edges / subgraphs.batch_size:.1f}\")\n",
    "    \n",
    "    # Show center node information\n",
    "    print(f\"\\nüéØ Center Node Information:\")\n",
    "    print(f\"   Center node IDs (first 5): {subgraphs.input_id[:5].tolist()}\")\n",
    "    print(f\"   Original node IDs (first 10): {subgraphs.n_id[:10].tolist()}\")\n",
    "    \n",
    "    # Calculate neighborhood expansion\n",
    "    expansion_ratio = subgraphs.num_nodes / subgraphs.batch_size\n",
    "    print(f\"\\nüìà Neighborhood Expansion:\")\n",
    "    print(f\"   Expansion ratio: {expansion_ratio:.2f}x\")\n",
    "    print(f\"   Each center node includes ~{expansion_ratio:.0f} nodes (itself + neighbors)\")\n",
    "    \n",
    "    print(f\"\\nüí° This batch contains {subgraphs.batch_size} subgraphs merged together\")\n",
    "    print(f\"   Each subgraph represents a {hop}-hop neighborhood around a center node\")\n",
    "    print(f\"   The model will process all {subgraphs.batch_size} subgraphs in parallel\")\n",
    "    \n",
    "    # Show the raw output for reference\n",
    "    print(f\"\\nüìù Raw Output:\")\n",
    "    print(subgraphs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Step-by-Step Data Processing Pipeline\n",
    "\n",
    "Now that we understand the target data structure, let's learn how to transform **raw spatial transcriptomics data** into the format required by Hist2Cell.\n",
    "\n",
    "## üìÇ Raw Data Structure Overview\n",
    "\n",
    "We've provided example raw data for slide `WSA_LngSP9258467` in the directory:\n",
    "```\n",
    "./example_data/example_raw_data/WSA_LngSP9258467/\n",
    "```\n",
    "\n",
    "This represents a typical spatial transcriptomics dataset with all the necessary components for Hist2Cell processing.\n",
    "\n",
    "## üìÅ Raw Data Components Explained\n",
    "\n",
    "### üñºÔ∏è **Image Files**\n",
    "| File | Purpose | Description |\n",
    "|------|---------|-------------|\n",
    "| `WSA_LngSP9258467.jpg` | Original slide image | High-resolution whole slide image (WSI) |\n",
    "| `WSA_LngSP9258467_low_res.jpg` | Low-resolution image | Quick visualization and processing |\n",
    "| `spot_view.jpg` | Spot visualization | Original slide with spot locations marked |\n",
    "| `2x_spot_view.jpg` | Super-resolution spots | 2x resolution spot visualization |\n",
    "\n",
    "### üìä **Gene Expression Data**\n",
    "| File | Content | Format | Use Case |\n",
    "|------|---------|--------|----------|\n",
    "| `stdata.csv` | Raw gene expression | Genes √ó Spots | Original measurements |\n",
    "| `log1p_stdata.csv` | Log-normalized expression | Genes √ó Spots | Processed for analysis |\n",
    "| `high_250_stdata.csv` | Top 250 genes | 250 √ó Spots | Highly expressed genes |\n",
    "| `high_250_stdata_log1p.csv` | Log-normalized top 250 | 250 √ó Spots | Ready for training |\n",
    "\n",
    "### üéØ **Cell Type Information**\n",
    "| File | Content | Description |\n",
    "|------|---------|-------------|\n",
    "| `cell_ratio.csv` | Cell type abundances | 80 fine-grained cell types per spot |\n",
    "\n",
    "### üìç **Spatial Information**\n",
    "| File | Content | Description |\n",
    "|------|---------|-------------|\n",
    "| `spots.csv` | Spot coordinates | X,Y pixel positions on slide |\n",
    "| `2x_spots.csv` | Super-resolution coordinates | 2x resolution positions |\n",
    "\n",
    "### üî≤ **Image Patches**\n",
    "| Folder | Content | Description |\n",
    "|--------|---------|-------------|\n",
    "| `patches/` | Image patches | 224√ó224 patches around each spot |\n",
    "| `2x_patches/` | Super-resolution patches | 2x resolution patches |\n",
    "\n",
    "## üîß **Image Patch Extraction**\n",
    "\n",
    "**Important**: To extract image patches from whole slide images (WSI), we recommend using the **DSMIL pipeline**:\n",
    "- **Repository**: [DSMIL-WSI](https://github.com/binli123/dsmil-wsi)\n",
    "- **Function**: Automatically crops patches around spot coordinates\n",
    "- **Output**: 224√ó224 pixel patches suitable for deep learning\n",
    "\n",
    "### üèóÔ∏è **Processing Workflow**\n",
    "\n",
    "The data processing pipeline follows these steps:\n",
    "\n",
    "1. **Image Patch Extraction**: Extract 224√ó224 patches from WSI around each spot\n",
    "2. **Gene Expression Processing**: Normalize and filter gene expression data\n",
    "3. **Cell Type Deconvolution**: Estimate cell type abundances (using tools like Cell2location)\n",
    "4. **Spatial Graph Construction**: Build neighborhood graphs based on spot positions\n",
    "5. **Data Integration**: Combine all components into PyTorch Geometric format\n",
    "\n",
    "Let's implement this pipeline step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Creating a Custom Dataset Class\n",
    "\n",
    "To efficiently process our raw spatial transcriptomics data, we'll create a custom `STDataset` class that handles:\n",
    "\n",
    "### üéØ **STDataset Functionality**\n",
    "\n",
    "1. **Data Loading**: Automatically loads image patches and corresponding labels\n",
    "2. **Data Validation**: Ensures image patches and labels are properly aligned\n",
    "3. **Preprocessing**: Applies image transformations (resize, normalize, etc.)\n",
    "4. **Integration**: Combines gene expression and cell type abundance data\n",
    "\n",
    "### üîß **Key Features**\n",
    "- **Automatic Matching**: Matches image patches with corresponding gene/cell data\n",
    "- **Flexible Transforms**: Supports PyTorch transforms for image preprocessing\n",
    "- **Error Handling**: Handles missing data gracefully\n",
    "- **Memory Efficient**: Loads data on-demand during iteration\n",
    "\n",
    "### üìä **Input Files Used**\n",
    "- `patches/`: Image patches (224√ó224 pixels)\n",
    "- `high_250_stdata.csv`: Top 250 gene expression values\n",
    "- `cell_ratio.csv`: Cell type abundance ratios\n",
    "\n",
    "Let's implement the STDataset class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data processing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "print(\"üì¶ Creating STDataset class for processing spatial transcriptomics data...\")\n",
    "\n",
    "class STDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for processing spatial transcriptomics data.\n",
    "    \n",
    "    This class handles loading and preprocessing of:\n",
    "    - Image patches from histology slides\n",
    "    - Gene expression data (top 250 genes)\n",
    "    - Cell type abundance labels (80 cell types)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, slide, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the STDataset.\n",
    "        \n",
    "        Args:\n",
    "            root (str): Root directory containing the raw data\n",
    "            slide (str): Slide name (e.g., 'WSA_LngSP9258467')\n",
    "            transform: Optional transforms to apply to images\n",
    "        \"\"\"\n",
    "        super(STDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.slide = slide\n",
    "        self.transform = transform\n",
    "        \n",
    "        print(f\"   üìÅ Loading data from: {os.path.join(root, slide)}\")\n",
    "\n",
    "        # 1. Load available image patches\n",
    "        patch_path = os.path.join(root, slide, 'patches')\n",
    "        patch_files = os.listdir(patch_path)\n",
    "        patch_list = [x.split('.')[0] for x in patch_files]  # Remove .jpg extension\n",
    "        print(f\"   üñºÔ∏è  Found {len(patch_list)} image patches\")\n",
    "\n",
    "        # 2. Load cell type abundance labels (80 fine-grained cell types)\n",
    "        cell_label_path = os.path.join(root, slide, 'cell_ratio.csv')\n",
    "        cell_label = pd.read_csv(cell_label_path, index_col=0)\n",
    "        print(f\"   üéØ Loaded cell type data: {cell_label.shape[1]} cell types\")\n",
    "        \n",
    "        # 3. Load gene expression labels (top 250 genes)\n",
    "        gene_label_path = os.path.join(root, slide, 'high_250_stdata.csv')\n",
    "        gene_label = pd.read_csv(gene_label_path, index_col=0)\n",
    "        print(f\"   üß¨ Loaded gene expression data: {gene_label.shape[1]} genes\")\n",
    "\n",
    "        # 4. Merge gene and cell type labels\n",
    "        label_df = pd.merge(gene_label, cell_label, left_index=True, right_index=True)\n",
    "        print(f\"   üîó Combined labels: {label_df.shape[1]} total (250 genes + 80 cell types)\")\n",
    "\n",
    "        # 5. Find intersection of spots with both images and labels\n",
    "        label_index_set = set(label_df.index)\n",
    "        patch_index_set = set(patch_list)\n",
    "        valid_spots = label_index_set & patch_index_set\n",
    "        \n",
    "        print(f\"   ‚úÖ Valid spots (with both image and labels): {len(valid_spots)}\")\n",
    "        if len(valid_spots) < len(patch_list):\n",
    "            print(f\"   ‚ö†Ô∏è  Missing labels for {len(patch_list) - len(valid_spots)} patches\")\n",
    "        if len(valid_spots) < len(label_df):\n",
    "            print(f\"   ‚ö†Ô∏è  Missing images for {len(label_df) - len(valid_spots)} labels\")\n",
    "\n",
    "        # 6. Store final data\n",
    "        self.patch = list(valid_spots)\n",
    "        self.label_df = label_df.loc[self.patch]\n",
    "        \n",
    "        print(f\"   üìä Dataset ready: {len(self.patch)} spots with complete data\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a single data sample.\n",
    "        \n",
    "        Args:\n",
    "            index (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (patch_id, image_data, labels)\n",
    "        \"\"\"\n",
    "        # Get spot ID\n",
    "        patch_id = self.patch[index]\n",
    "        \n",
    "        # Load and preprocess image patch\n",
    "        patch_path = os.path.join(self.root, self.slide, 'patches', patch_id + '.jpg')\n",
    "        patch = Image.open(patch_path).convert('RGB')\n",
    "        \n",
    "        # Resize to 224x224 (required for ResNet18)\n",
    "        data = transforms.Resize((224, 224))(patch)\n",
    "        \n",
    "        # Apply additional transforms if provided\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        # Get corresponding labels (250 genes + 80 cell types)\n",
    "        label = self.label_df.loc[patch_id].values\n",
    "        label = torch.Tensor(label)\n",
    "\n",
    "        return patch_id, data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.patch)\n",
    "\n",
    "print(\"‚úÖ STDataset class defined successfully!\")\n",
    "print(\"   This class will automatically handle data loading and preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Step 2: Setting up data transformations and dataset...\")\n",
    "\n",
    "# Define image preprocessing pipeline\n",
    "print(\"   üìã Configuring image transformations:\")\n",
    "print(\"      - Convert PIL Image to PyTorch tensor\")\n",
    "print(\"      - Normalize with ImageNet statistics (required for ResNet18)\")\n",
    "print(\"      - Mean: [0.485, 0.456, 0.406] (ImageNet RGB means)\")\n",
    "print(\"      - Std:  [0.229, 0.224, 0.225] (ImageNet RGB stds)\")\n",
    "\n",
    "test_transform_pcam = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor and scale to [0,1]\n",
    "    transforms.Normalize(   # Normalize with ImageNet statistics\n",
    "        mean=[0.485, 0.456, 0.406],  # RGB channel means\n",
    "        std=[0.229, 0.224, 0.225]    # RGB channel standard deviations\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"\\nüèóÔ∏è Creating dataset instance...\")\n",
    "# Create dataset instance\n",
    "test_data = STDataset(\n",
    "    root=\"../example_data/example_raw_data\", \n",
    "    slide=\"WSA_LngSP9258467\",\n",
    "    transform=test_transform_pcam\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total samples: {len(test_data)}\")\n",
    "print(f\"   Data source: ../example_data/example_raw_data/WSA_LngSP9258467\")\n",
    "print(f\"   Transform applied: ImageNet normalization\")\n",
    "\n",
    "print(\"\\nüîÑ Creating DataLoader...\")\n",
    "# Create DataLoader for batch processing\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=512,      # Process 512 spots at once\n",
    "    shuffle=False,       # Maintain order for reproducibility\n",
    "    num_workers=4        # Use 4 parallel workers for faster loading\n",
    ")\n",
    "\n",
    "print(f\"   DataLoader configuration:\")\n",
    "print(f\"      Batch size: 512 spots\")\n",
    "print(f\"      Total batches: {len(test_loader)}\")\n",
    "print(f\"      Parallel workers: 4\")\n",
    "print(f\"      Shuffle: False (maintains spot order)\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset and DataLoader ready for processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Processing and Collecting Data\n",
    "\n",
    "Now we'll iterate through our `STDataset` to collect all the necessary components:\n",
    "\n",
    "### üéØ **Data Collection Process**\n",
    "\n",
    "1. **Image Patches**: Extract preprocessed 224√ó224 image patches\n",
    "2. **Labels**: Collect gene expression + cell type abundance labels  \n",
    "3. **Spot IDs**: Maintain spot identifiers for later graph construction\n",
    "\n",
    "### üîß **Processing Steps**\n",
    "\n",
    "- **Batch Processing**: Process data in batches of 512 for efficiency\n",
    "- **Memory Management**: Handle varying batch sizes gracefully\n",
    "- **Data Validation**: Ensure all arrays have consistent shapes\n",
    "- **Progress Tracking**: Monitor processing progress\n",
    "\n",
    "### üìã **Expected Output**\n",
    "\n",
    "After processing, we'll have:\n",
    "- `spot_data_array`: Shape [num_spots, 3, 224, 224] - Image patches\n",
    "- `spot_label_array`: Shape [num_spots, 330] - Gene + cell type labels\n",
    "- `spot_id_array`: List of spot identifiers\n",
    "\n",
    "Let's process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Processing data through DataLoader...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize storage arrays\n",
    "spot_data_array = []\n",
    "spot_label_array = []\n",
    "spot_id_array = []\n",
    "\n",
    "# Process data in batches\n",
    "batch_count = 0\n",
    "total_spots_processed = 0\n",
    "\n",
    "for name, data, label in test_loader:\n",
    "    batch_count += 1\n",
    "    batch_size = len(name)\n",
    "    total_spots_processed += batch_size\n",
    "    \n",
    "    print(f\"   üì¶ Processing batch {batch_count}/{len(test_loader)}: {batch_size} spots\")\n",
    "    \n",
    "    # Store batch data\n",
    "    spot_id_array.append(list(name))\n",
    "    \n",
    "    # Process labels (ensure float type and proper shape)\n",
    "    label = label.float()\n",
    "    label = label.squeeze()\n",
    "    spot_label_array.append(label.detach().numpy())\n",
    "    \n",
    "    # Store image data\n",
    "    spot_data_array.append(data.detach().numpy())\n",
    "\n",
    "print(f\"‚úÖ Batch processing complete! Processed {total_spots_processed} spots\")\n",
    "\n",
    "print(\"\\nüîß Post-processing data arrays...\")\n",
    "\n",
    "# Handle single-item batches by adding batch dimension\n",
    "print(\"   üìè Ensuring consistent array dimensions...\")\n",
    "for i in range(len(spot_data_array)):\n",
    "    if len(spot_data_array[i].shape) <= 1:\n",
    "        spot_data_array[i] = spot_data_array[i][np.newaxis, :]\n",
    "        print(f\"      Fixed data array {i} shape\")\n",
    "\n",
    "for i in range(len(spot_label_array)):\n",
    "    if len(spot_label_array[i].shape) <= 1:\n",
    "        spot_label_array[i] = spot_label_array[i][np.newaxis, :]\n",
    "        print(f\"      Fixed label array {i} shape\")\n",
    "\n",
    "# Concatenate all batch arrays into final arrays\n",
    "print(\"   üîó Concatenating batch arrays...\")\n",
    "spot_data_array = np.concatenate(spot_data_array)\n",
    "spot_label_array = np.concatenate(spot_label_array)\n",
    "\n",
    "# Flatten spot ID list\n",
    "spot_ids = []\n",
    "for ids in spot_id_array:\n",
    "    spot_ids.extend(ids)\n",
    "spot_id_array = spot_ids\n",
    "\n",
    "print(f\"‚úÖ Data processing complete!\")\n",
    "print(f\"   üìä Final array shapes:\")\n",
    "print(f\"      Image data: {spot_data_array.shape}\")\n",
    "print(f\"      Label data: {spot_label_array.shape}\")\n",
    "print(f\"      Spot IDs: {len(spot_id_array)} identifiers\")\n",
    "print(f\"   üéØ Data validation:\")\n",
    "print(f\"      All arrays have {spot_data_array.shape[0]} spots\")\n",
    "print(f\"      Labels contain {spot_label_array.shape[1]} values (250 genes + 80 cell types)\")\n",
    "print(f\"      Image patches are {spot_data_array.shape[1]}√ó{spot_data_array.shape[2]}√ó{spot_data_array.shape[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final processed data shape\n",
    "print(\"üìä Final Processed Data Summary:\")\n",
    "print(f\"   Image data shape: {spot_data_array.shape}\")\n",
    "print(f\"   Format: [num_spots, channels, height, width]\")\n",
    "print(f\"   Spots: {spot_data_array.shape[0]}\")\n",
    "print(f\"   Channels: {spot_data_array.shape[1]} (RGB)\")\n",
    "print(f\"   Image size: {spot_data_array.shape[2]}√ó{spot_data_array.shape[3]} pixels\")\n",
    "print(f\"   Memory usage: {spot_data_array.nbytes / (1024**2):.1f} MB\")\n",
    "\n",
    "# Show the actual shape for reference\n",
    "spot_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample spot IDs and their structure\n",
    "print(\"üîç Spot ID Analysis:\")\n",
    "print(f\"   Total spot IDs: {len(spot_id_array)}\")\n",
    "print(f\"   ID format: slide_spotbarcode\")\n",
    "print(f\"   Sample IDs (first 5):\")\n",
    "for i, spot_id in enumerate(spot_id_array[:5]):\n",
    "    print(f\"      {i}: {spot_id}\")\n",
    "\n",
    "# Analyze ID structure\n",
    "sample_id = spot_id_array[0]\n",
    "slide_name = sample_id.split('_')[0] + '_' + sample_id.split('_')[1]\n",
    "barcode = sample_id.split('_')[2]\n",
    "print(f\"\\nüìã ID Structure Analysis:\")\n",
    "print(f\"   Slide name: {slide_name}\")\n",
    "print(f\"   Barcode format: {barcode}\")\n",
    "print(f\"   Example full ID: {sample_id}\")\n",
    "\n",
    "print(f\"\\nüí° These IDs will be used to:\")\n",
    "print(f\"   - Match spots with spatial coordinates\")\n",
    "print(f\"   - Build spatial neighborhood graphs\")\n",
    "print(f\"   - Track data provenance\")\n",
    "\n",
    "# Show the actual first 5 IDs for reference\n",
    "spot_id_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìç Step 4: Loading Spatial Coordinate Information\n",
    "\n",
    "To build the spatial neighborhood graph, we need to understand the **spatial layout** of spots on the tissue slide. \n",
    "\n",
    "### üéØ **Coordinate Systems in Spatial Transcriptomics**\n",
    "\n",
    "Spatial transcriptomics data uses **two coordinate systems**:\n",
    "\n",
    "1. **Array Coordinates** (`array_col`, `array_row`):\n",
    "   - **Grid-based positions** on the regular spot array\n",
    "   - **Integer values** representing row/column positions\n",
    "   - **Used for**: Determining spatial neighbors (adjacency)\n",
    "   - **Example**: (col=27, row=29) means spot is in column 27, row 29 of the grid\n",
    "\n",
    "2. **Pixel Coordinates** (`x`, `y`):\n",
    "   - **Actual pixel positions** on the histology image\n",
    "   - **Continuous values** in pixel units\n",
    "   - **Used for**: Visualization and precise spatial analysis\n",
    "   - **Example**: (x=12701, y=9136) means spot is at pixel (12701, 9136)\n",
    "\n",
    "### üîó **Why Array Coordinates Matter**\n",
    "\n",
    "- **Neighborhood Definition**: Spots are neighbors if their array coordinates are adjacent\n",
    "- **Graph Construction**: Edges connect spots with nearby array positions\n",
    "- **Regular Grid Structure**: Spatial transcriptomics uses hexagonal/square grid patterns\n",
    "- **Consistent Spacing**: Array coordinates ensure uniform spatial relationships\n",
    "\n",
    "### üìä **Data Source**\n",
    "\n",
    "We'll load array coordinates from a pre-processed AnnData file that contains spatial metadata for all spots.\n",
    "\n",
    "Let's load the spatial coordinate information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scanpy for reading spatial transcriptomics data\n",
    "import scanpy as sc\n",
    "\n",
    "print(\"üìç Loading spatial coordinate metadata...\")\n",
    "\n",
    "# Load the AnnData file containing spatial metadata\n",
    "adata_path = \"../example_data/example_raw_data/sp.X_norm5e4_log1p.h5ad\"\n",
    "print(f\"   üìÅ Loading from: {adata_path}\")\n",
    "\n",
    "adata = sc.read(adata_path)\n",
    "\n",
    "# Extract array coordinates (grid positions)\n",
    "spot_array_cols = adata.obs.array_col\n",
    "spot_array_rows = adata.obs.array_row\n",
    "\n",
    "print(f\"‚úÖ Spatial metadata loaded successfully!\")\n",
    "print(f\"   üìä Dataset information:\")\n",
    "print(f\"      Total spots in AnnData: {adata.n_obs}\")\n",
    "print(f\"      Array column range: {spot_array_cols.min()} to {spot_array_cols.max()}\")\n",
    "print(f\"      Array row range: {spot_array_rows.min()} to {spot_array_rows.max()}\")\n",
    "print(f\"      Grid dimensions: {spot_array_cols.max() - spot_array_cols.min() + 1} √ó {spot_array_rows.max() - spot_array_rows.min() + 1}\")\n",
    "\n",
    "print(f\"\\nüìã Coordinate format:\")\n",
    "print(f\"   Index: Spot barcode (e.g., 'WSA_LngSP8759311_AAACAAGTATCTCCCA-1')\")\n",
    "print(f\"   array_col: Column position in spatial grid\")\n",
    "print(f\"   array_row: Row position in spatial grid\")\n",
    "\n",
    "print(f\"\\nüí° These coordinates will be used to:\")\n",
    "print(f\"   - Determine spatial neighbors for each spot\")\n",
    "print(f\"   - Build adjacency matrix for graph construction\")\n",
    "print(f\"   - Ensure consistent spatial relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_array_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 5: Graph Construction and Spatial Relationships\n",
    "\n",
    "### üéØ **Understanding Spatial Neighborhood Patterns**\n",
    "\n",
    "From the spot visualization image (`./example_data/example_raw_data/WSA_LngSP9258467/spot_view.jpg`), we can observe that each spot has **6 nearest neighbors** in a hexagonal pattern. This is characteristic of spatial transcriptomics platforms like Visium.\n",
    "\n",
    "### üìê **Hexagonal Grid Structure**\n",
    "\n",
    "In spatial transcriptomics:\n",
    "- **Hexagonal Pattern**: Spots are arranged in a hexagonal grid\n",
    "- **6 Neighbors**: Each spot (except edges) has exactly 6 spatial neighbors\n",
    "- **Regular Spacing**: Neighbors are at consistent distances\n",
    "- **Symmetric Relations**: If A is a neighbor of B, then B is a neighbor of A\n",
    "\n",
    "### üîß **Neighborhood Definition Algorithm**\n",
    "\n",
    "We'll create the adjacency matrix based on array coordinates:\n",
    "\n",
    "```python\n",
    "for each spot i:\n",
    "    for each spot j:\n",
    "        if distance(array_coords[i], array_coords[j]) <= threshold:\n",
    "            adjacency[i][j] = 1  # They are neighbors\n",
    "```\n",
    "\n",
    "### üéöÔ∏è **Distance Thresholds**\n",
    "\n",
    "Based on the hexagonal grid pattern:\n",
    "- **Column distance**: ¬±2 (within 2 columns)\n",
    "- **Row distance**: ¬±1 (within 1 row)\n",
    "- **Self-connection**: Each spot connects to itself (diagonal = 1)\n",
    "\n",
    "### üìä **Expected Graph Properties**\n",
    "\n",
    "After construction, we expect:\n",
    "- **Nodes**: 422 spots (same as our processed data)\n",
    "- **Edges**: ~2,600-2,800 edges (each spot has ~6 neighbors)\n",
    "- **Symmetry**: Undirected graph (mutual connections)\n",
    "- **Connectivity**: All spots should be connected (single component)\n",
    "\n",
    "Let's build the spatial graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_array_x_y = []\n",
    "for item in spot_id_array:\n",
    "    spot_array_x_y.append([int(spot_array_cols[item]), int(spot_array_rows[item])])\n",
    "    \n",
    "spot_array_x_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Building spatial adjacency matrix...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize adjacency matrix\n",
    "num_spots = len(spot_array_x_y)\n",
    "adj = np.zeros((num_spots, num_spots))\n",
    "\n",
    "print(f\"   üìä Matrix dimensions: {num_spots} √ó {num_spots}\")\n",
    "print(f\"   üîß Neighborhood criteria:\")\n",
    "print(f\"      - Column distance: within ¬±2 positions\")\n",
    "print(f\"      - Row distance: within ¬±1 positions\")\n",
    "print(f\"      - Self-connections: included (diagonal = 1)\")\n",
    "\n",
    "# Build adjacency matrix with progress tracking\n",
    "connections_made = 0\n",
    "progress_interval = num_spots // 10  # Show progress every 10%\n",
    "\n",
    "for i in range(num_spots):\n",
    "    # Show progress\n",
    "    if i % progress_interval == 0:\n",
    "        progress = (i / num_spots) * 100\n",
    "        print(f\"   üìà Progress: {progress:.0f}% ({i}/{num_spots} spots processed)\")\n",
    "    \n",
    "    for j in range(num_spots):\n",
    "        if i == j:\n",
    "            # Self-connection\n",
    "            adj[i][j] = 1.0\n",
    "            connections_made += 1\n",
    "        else:\n",
    "            # Get array coordinates\n",
    "            x1, y1 = spot_array_x_y[i]\n",
    "            x2, y2 = spot_array_x_y[j]\n",
    "            \n",
    "            # Check if spots are neighbors based on hexagonal grid pattern\n",
    "            col_dist = abs(x2 - x1)\n",
    "            row_dist = abs(y2 - y1)\n",
    "            \n",
    "            # Hexagonal neighborhood: within 2 columns and 1 row\n",
    "            if col_dist < 3 and row_dist < 2:\n",
    "                adj[i][j] = 1.0\n",
    "                connections_made += 1\n",
    "\n",
    "print(f\"‚úÖ Adjacency matrix construction complete!\")\n",
    "print(f\"   üìä Graph statistics:\")\n",
    "print(f\"      Total possible connections: {num_spots * num_spots:,}\")\n",
    "print(f\"      Actual connections made: {connections_made:,}\")\n",
    "print(f\"      Graph density: {(connections_made / (num_spots * num_spots)) * 100:.2f}%\")\n",
    "print(f\"      Average degree: {connections_made / num_spots:.2f}\")\n",
    "print(f\"      Expected edges in PyTorch Geometric: {connections_made // 2:,} (undirected)\")\n",
    "\n",
    "# Verify symmetry (should be symmetric for undirected graph)\n",
    "is_symmetric = np.allclose(adj, adj.T)\n",
    "print(f\"   ‚úÖ Graph symmetry check: {'PASSED' if is_symmetric else 'FAILED'}\")\n",
    "\n",
    "print(f\"\\nüí° This adjacency matrix defines the spatial relationships\")\n",
    "print(f\"   Each 1 indicates two spots are spatial neighbors\")\n",
    "print(f\"   This will be converted to edge_index format for PyTorch Geometric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìç Step 6: Loading Pixel Coordinates\n",
    "\n",
    "In addition to array coordinates for graph construction, we need **pixel coordinates** for visualization and spatial analysis.\n",
    "\n",
    "### üéØ **Pixel Coordinates vs Array Coordinates**\n",
    "\n",
    "| Coordinate Type | Purpose | Format | Example |\n",
    "|----------------|---------|---------|---------|\n",
    "| **Array Coordinates** | Graph construction | Grid positions (int) | (27, 29) |\n",
    "| **Pixel Coordinates** | Visualization & analysis | Image positions (float) | (12701.5, 9136.2) |\n",
    "\n",
    "### üìä **Uses of Pixel Coordinates**\n",
    "\n",
    "- **Visualization**: Plotting predictions on tissue slides\n",
    "- **Spatial Analysis**: Calculating distances between spots\n",
    "- **Co-localization**: Measuring spatial relationships between cell types\n",
    "- **Quality Control**: Verifying spatial patterns\n",
    "\n",
    "### üìÅ **Data Source**\n",
    "\n",
    "The pixel coordinates are stored in `spots.csv` and contain the exact (x,y) positions of each spot on the original histology slide.\n",
    "\n",
    "Let's load the pixel coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìç Loading pixel coordinates...\")\n",
    "\n",
    "# Load pixel coordinates from spots.csv\n",
    "spots_coord_path = \"../example_data/example_raw_data/WSA_LngSP9258467/spots.csv\"\n",
    "print(f\"   üìÅ Loading from: {spots_coord_path}\")\n",
    "\n",
    "spots_coord_df = pd.read_csv(spots_coord_path, index_col=0)\n",
    "print(f\"   üìä Loaded coordinates for {len(spots_coord_df)} spots\")\n",
    "\n",
    "# Filter to only spots that we have processed data for\n",
    "print(f\"   üîç Filtering to {len(spot_id_array)} processed spots...\")\n",
    "spots_coord = spots_coord_df.loc[spot_id_array].values\n",
    "\n",
    "# Validate coordinate data\n",
    "print(f\"   ‚úÖ Coordinate validation:\")\n",
    "print(f\"      Shape: {spots_coord.shape}\")\n",
    "print(f\"      X range: {spots_coord[:, 0].min():.0f} to {spots_coord[:, 0].max():.0f}\")\n",
    "print(f\"      Y range: {spots_coord[:, 1].min():.0f} to {spots_coord[:, 1].max():.0f}\")\n",
    "print(f\"      Data type: {spots_coord.dtype}\")\n",
    "\n",
    "# Check for any missing coordinates\n",
    "missing_coords = len(spot_id_array) - len(spots_coord)\n",
    "if missing_coords > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {missing_coords} spots missing pixel coordinates\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ All {len(spot_id_array)} spots have pixel coordinates\")\n",
    "\n",
    "# Calculate spatial extent\n",
    "width = spots_coord[:, 0].max() - spots_coord[:, 0].min()\n",
    "height = spots_coord[:, 1].max() - spots_coord[:, 1].min()\n",
    "print(f\"   üìè Spatial extent: {width:.0f} √ó {height:.0f} pixels\")\n",
    "\n",
    "# Show sample coordinates\n",
    "print(f\"\\nüìã Sample pixel coordinates:\")\n",
    "for i in range(min(5, len(spots_coord))):\n",
    "    spot_id = spot_id_array[i]\n",
    "    x, y = spots_coord[i]\n",
    "    print(f\"      {spot_id}: ({x:.0f}, {y:.0f})\")\n",
    "\n",
    "print(f\"\\nüí° These coordinates will be used for:\")\n",
    "print(f\"   - Visualizing predictions on tissue slides\")\n",
    "print(f\"   - Calculating spatial distances and relationships\")\n",
    "print(f\"   - Quality control and validation\")\n",
    "\n",
    "# Show the first 5 coordinates for reference\n",
    "spots_coord[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 7: Creating PyTorch Geometric Data Object\n",
    "\n",
    "Finally, we'll combine all processed components into a single `torch_geometric.data.Data` object that Hist2Cell can use for training and inference.\n",
    "\n",
    "### üì¶ **Data Integration Process**\n",
    "\n",
    "We'll combine the following components:\n",
    "\n",
    "| Component | Variable | Shape | Description |\n",
    "|-----------|----------|-------|-------------|\n",
    "| **Image Features** | `x` | [422, 3, 224, 224] | Preprocessed histology patches |\n",
    "| **Labels** | `y` | [422, 330] | Gene expression + cell abundances |\n",
    "| **Graph Structure** | `edge_index` | [2, num_edges] | Spatial connectivity |\n",
    "| **Positions** | `pos` | [422, 2] | Pixel coordinates |\n",
    "| **Metadata** | `spot_id` | [422] | Spot identifiers |\n",
    "\n",
    "### üîß **PyTorch Geometric Format**\n",
    "\n",
    "The final `Data` object will contain:\n",
    "- **Node features (`x`)**: Image patches for each spot\n",
    "- **Edge indices (`edge_index`)**: Spatial graph connectivity in COO format\n",
    "- **Labels (`y`)**: Training targets (genes + cell types)\n",
    "- **Positions (`pos`)**: Pixel coordinates for visualization\n",
    "- **Metadata (`spot_id`)**: Spot identifiers for tracking\n",
    "\n",
    "### üéØ **Key Transformations**\n",
    "\n",
    "1. **Adjacency ‚Üí Edge Index**: Convert adjacency matrix to COO format\n",
    "2. **Numpy ‚Üí PyTorch**: Convert all arrays to PyTorch tensors\n",
    "3. **Data Validation**: Ensure all components have consistent dimensions\n",
    "4. **Graph Properties**: Verify graph connectivity and structure\n",
    "\n",
    "### üíæ **Output**\n",
    "\n",
    "The final processed data will be saved as:\n",
    "- **Format**: `.pt` file (PyTorch format)\n",
    "- **Size**: ~50-100 MB (depends on number of spots)\n",
    "- **Structure**: Ready for Hist2Cell training/inference\n",
    "\n",
    "Let's create the final data object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Creating PyTorch Geometric Data object...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import required modules\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Data\n",
    "from torch import Tensor\n",
    "\n",
    "print(\"üì¶ Converting data components to PyTorch tensors...\")\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "print(\"   üñºÔ∏è  Converting image data...\")\n",
    "x = Tensor(spot_data_array)\n",
    "print(f\"      Shape: {x.shape}\")\n",
    "print(f\"      Memory: {x.numel() * x.element_size() / (1024**2):.1f} MB\")\n",
    "\n",
    "print(\"   üè∑Ô∏è  Converting label data...\")\n",
    "y = Tensor(spot_label_array)\n",
    "print(f\"      Shape: {y.shape}\")\n",
    "print(f\"      Components: {y.shape[1]} (250 genes + 80 cell types)\")\n",
    "\n",
    "print(\"   üìç Converting position data...\")\n",
    "pos = Tensor(spots_coord)\n",
    "print(f\"      Shape: {pos.shape}\")\n",
    "print(f\"      X range: {pos[:, 0].min():.0f} to {pos[:, 0].max():.0f}\")\n",
    "print(f\"      Y range: {pos[:, 1].min():.0f} to {pos[:, 1].max():.0f}\")\n",
    "\n",
    "print(\"   üîó Converting adjacency matrix to edge index...\")\n",
    "adj_tensor = Tensor(adj)\n",
    "edge_index, edge_weights = dense_to_sparse(adj_tensor)\n",
    "print(f\"      Adjacency matrix shape: {adj_tensor.shape}\")\n",
    "print(f\"      Edge index shape: {edge_index.shape}\")\n",
    "print(f\"      Number of edges: {edge_index.shape[1]}\")\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "print(\"\\nüèóÔ∏è Creating PyTorch Geometric Data object...\")\n",
    "data = Data(\n",
    "    x=x,                    # Node features (image patches)\n",
    "    edge_index=edge_index,  # Graph connectivity\n",
    "    y=y,                    # Labels (genes + cell types)\n",
    "    pos=pos,                # Pixel coordinates\n",
    "    spot_id=spot_id_array   # Spot identifiers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data object created successfully!\")\n",
    "print(f\"   üìä Final data structure:\")\n",
    "print(f\"      Nodes: {data.num_nodes}\")\n",
    "print(f\"      Edges: {data.num_edges}\")\n",
    "print(f\"      Node features: {data.x.shape}\")\n",
    "print(f\"      Labels: {data.y.shape}\")\n",
    "print(f\"      Positions: {data.pos.shape}\")\n",
    "print(f\"      Spot IDs: {len(data.spot_id)}\")\n",
    "\n",
    "# Data validation checks\n",
    "print(f\"\\n‚úÖ Data validation:\")\n",
    "print(f\"   - Node consistency: {data.x.shape[0] == data.y.shape[0] == data.pos.shape[0] == len(data.spot_id)}\")\n",
    "print(f\"   - Edge validation: {data.edge_index.max().item() < data.num_nodes}\")\n",
    "print(f\"   - Feature dimensions: {data.x.shape[1:]} (3 channels, 224x224)\")\n",
    "print(f\"   - Label dimensions: {data.y.shape[1]} (330 total)\")\n",
    "\n",
    "# Memory usage summary\n",
    "total_memory = (data.x.numel() * data.x.element_size() + \n",
    "                data.y.numel() * data.y.element_size() + \n",
    "                data.pos.numel() * data.pos.element_size() + \n",
    "                data.edge_index.numel() * data.edge_index.element_size()) / (1024**2)\n",
    "print(f\"   - Total memory: {total_memory:.1f} MB\")\n",
    "\n",
    "print(f\"\\nüí° This data object is now ready for:\")\n",
    "print(f\"   - Hist2Cell model training\")\n",
    "print(f\"   - Inference and prediction\")\n",
    "print(f\"   - Subgraph sampling with NeighborLoader\")\n",
    "print(f\"   - Visualization and analysis\")\n",
    "\n",
    "# Show the final data object\n",
    "print(f\"\\nüìã Final Data Object:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving processed data to file...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define output path\n",
    "output_path = \"../example_data/example_processed_data/WSA_LngSP9258467.pt\"\n",
    "print(f\"   üìÅ Output path: {output_path}\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Save the data object\n",
    "print(\"   üîÑ Saving PyTorch Geometric Data object...\")\n",
    "torch.save(data, output_path)\n",
    "\n",
    "# Verify the saved file\n",
    "if os.path.exists(output_path):\n",
    "    file_size = os.path.getsize(output_path) / (1024**2)  # MB\n",
    "    print(f\"‚úÖ Data saved successfully!\")\n",
    "    print(f\"   üìä File information:\")\n",
    "    print(f\"      Path: {output_path}\")\n",
    "    print(f\"      Size: {file_size:.1f} MB\")\n",
    "    print(f\"      Format: PyTorch (.pt)\")\n",
    "    \n",
    "    # Test loading the saved data\n",
    "    print(f\"\\nüß™ Testing data loading...\")\n",
    "    try:\n",
    "        loaded_data = torch.load(output_path)\n",
    "        print(f\"   ‚úÖ Load test successful!\")\n",
    "        print(f\"      Loaded data: {loaded_data}\")\n",
    "        print(f\"      Nodes: {loaded_data.num_nodes}\")\n",
    "        print(f\"      Edges: {loaded_data.num_edges}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Load test failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Failed to save data to {output_path}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Use this data for Hist2Cell training:\")\n",
    "print(f\"      - See ../tutorial_training/training_tutorial.ipynb\")\n",
    "print(f\"   2. Process additional slides using the same pipeline\")\n",
    "print(f\"   3. Create train/test splits for model evaluation\")\n",
    "print(f\"   4. Analyze predictions with evaluation tutorials\")\n",
    "\n",
    "print(f\"\\nüí° This processed data contains:\")\n",
    "print(f\"   - Ready-to-use histology image patches\")\n",
    "print(f\"   - Spatial graph structure for neighboring relationships\")\n",
    "print(f\"   - Gene expression and cell type abundance labels\")\n",
    "print(f\"   - Pixel coordinates for visualization\")\n",
    "print(f\"   - All metadata needed for downstream analysis\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation tutorial complete!\")\n",
    "print(f\"   Your spatial transcriptomics data is now ready for Hist2Cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üéâ Tutorial Complete! Your Data is Ready for Hist2Cell\n",
    "\n",
    "## üìã Summary of What We've Accomplished\n",
    "\n",
    "Congratulations! You've successfully completed the **Hist2Cell Data Preparation Tutorial**. Here's what we've achieved:\n",
    "\n",
    "### ‚úÖ **Data Processing Pipeline**\n",
    "1. **üìä Data Structure Understanding**: Learned PyTorch Geometric format for spatial transcriptomics\n",
    "2. **üñºÔ∏è Image Processing**: Loaded and preprocessed 224√ó224 histology patches\n",
    "3. **üß¨ Label Integration**: Combined gene expression and cell type abundance data\n",
    "4. **üìç Spatial Mapping**: Extracted both array and pixel coordinates\n",
    "5. **üîó Graph Construction**: Built spatial neighborhood relationships\n",
    "6. **üì¶ Final Integration**: Created PyTorch Geometric Data object\n",
    "7. **üíæ Data Saving**: Saved processed data for training and inference\n",
    "\n",
    "### üìä **Final Data Structure**\n",
    "- **Nodes**: 422 spots with complete data\n",
    "- **Edges**: ~2,700 spatial connections\n",
    "- **Features**: 224√ó224 RGB image patches\n",
    "- **Labels**: 330 values (250 genes + 80 cell types)\n",
    "- **Coordinates**: Both array and pixel positions\n",
    "- **Format**: Ready for Hist2Cell training\n",
    "\n",
    "## üîÑ Next Steps in Your Hist2Cell Journey\n",
    "\n",
    "### 1. **üèãÔ∏è Model Training**\n",
    "- **Tutorial**: `../tutorial_training/training_tutorial.ipynb`\n",
    "- **Purpose**: Train Hist2Cell on your processed data\n",
    "- **Output**: Trained model weights for inference\n",
    "\n",
    "### 2. **üìà Analysis & Evaluation**\n",
    "Navigate to `../tutorial_analysis_evaluation/` for:\n",
    "- **Cell Abundance Visualization**: Visualize predictions on tissue\n",
    "- **Key Cell Evaluation**: Evaluate specific cell types of interest\n",
    "- **Cell Co-localization**: Analyze spatial relationships\n",
    "- **Super-resolution**: Generate high-resolution abundance maps\n",
    "\n",
    "### 3. **üîÑ Process Additional Data**\n",
    "Apply this pipeline to your own datasets:\n",
    "- **Multiple Slides**: Process entire experiments\n",
    "- **Different Tissues**: Adapt to various tissue types\n",
    "- **Custom Cell Types**: Modify cell type definitions\n",
    "- **Batch Processing**: Scale to large datasets\n",
    "\n",
    "## üõ†Ô∏è Troubleshooting Guide\n",
    "\n",
    "### ‚ùì Common Issues and Solutions\n",
    "\n",
    "#### **Issue**: Memory errors during processing\n",
    "- **Solution**: Reduce batch size in DataLoader\n",
    "- **Code**: `batch_size=256` instead of `batch_size=512`\n",
    "\n",
    "#### **Issue**: Missing image patches\n",
    "- **Solution**: Check patch extraction with DSMIL pipeline\n",
    "- **Reference**: [DSMIL-WSI](https://github.com/binli123/dsmil-wsi)\n",
    "\n",
    "#### **Issue**: Coordinate mismatches\n",
    "- **Solution**: Verify spot IDs match between image and label files\n",
    "- **Check**: Ensure consistent spot naming conventions\n",
    "\n",
    "#### **Issue**: Graph connectivity problems\n",
    "- **Solution**: Adjust distance thresholds in adjacency matrix\n",
    "- **Modify**: `col_dist < 3` and `row_dist < 2` parameters\n",
    "\n",
    "### üîß **Performance Optimization**\n",
    "\n",
    "#### **For Large Datasets**:\n",
    "- **Parallel Processing**: Increase `num_workers` in DataLoader\n",
    "- **Memory Management**: Process data in smaller chunks\n",
    "- **Storage**: Use SSD for faster I/O operations\n",
    "\n",
    "#### **For GPU Memory**:\n",
    "- **Reduce Batch Size**: Lower `subgraph_bs` in NeighborLoader\n",
    "- **Mixed Precision**: Use `torch.cuda.amp` for training\n",
    "- **Model Checkpointing**: Save memory during training\n",
    "\n",
    "## üìö Best Practices for Your Own Data\n",
    "\n",
    "### üéØ **Data Quality Checks**\n",
    "1. **Image Quality**: Ensure patches are clear and well-focused\n",
    "2. **Coordinate Accuracy**: Verify spatial positions are correct\n",
    "3. **Label Completeness**: Check for missing gene/cell type data\n",
    "4. **Graph Connectivity**: Ensure all spots are properly connected\n",
    "\n",
    "### üìè **Standardization Tips**\n",
    "1. **Image Normalization**: Always use ImageNet statistics\n",
    "2. **Coordinate Systems**: Maintain consistent spatial references\n",
    "3. **Spot Naming**: Use consistent barcode formats\n",
    "4. **File Organization**: Keep organized directory structures\n",
    "\n",
    "### üîÑ **Batch Processing Workflow**\n",
    "```python\n",
    "# Example batch processing loop\n",
    "slides = ['slide1', 'slide2', 'slide3']\n",
    "for slide in slides:\n",
    "    # Process each slide using the pipeline\n",
    "    processed_data = process_slide(slide)\n",
    "    torch.save(processed_data, f'processed_{slide}.pt')\n",
    "```\n",
    "\n",
    "## üåü Advanced Usage Tips\n",
    "\n",
    "### üé® **Custom Cell Types**\n",
    "- **Cell2location**: Use custom reference for deconvolution\n",
    "- **scRNA-seq**: Integrate your own single-cell reference\n",
    "- **Manual Annotation**: Include expert annotations\n",
    "\n",
    "### üî¨ **Multi-modal Integration**\n",
    "- **Protein Data**: Add protein abundance information\n",
    "- **Clinical Data**: Include patient metadata\n",
    "- **Temporal Data**: Process time-series experiments\n",
    "\n",
    "### üöÄ **Performance Scaling**\n",
    "- **GPU Acceleration**: Use CUDA for faster processing\n",
    "- **Distributed Processing**: Scale across multiple GPUs\n",
    "- **Cloud Computing**: Leverage cloud resources for large datasets\n",
    "\n",
    "## üìñ Additional Resources\n",
    "\n",
    "### üìù **Documentation**\n",
    "- **PyTorch Geometric**: [https://pytorch-geometric.readthedocs.io/](https://pytorch-geometric.readthedocs.io/)\n",
    "- **Scanpy**: [https://scanpy.readthedocs.io/](https://scanpy.readthedocs.io/)\n",
    "- **Cell2location**: [https://cell2location.readthedocs.io/](https://cell2location.readthedocs.io/)\n",
    "\n",
    "### üî¨ **Research Papers**\n",
    "- **Hist2Cell**: [Deciphering Fine-grained Cellular Architectures from Histology Images](https://www.biorxiv.org/content/10.1101/2024.02.17.580852v1.full.pdf)\n",
    "- **Spatial Transcriptomics**: Key papers in spatial biology\n",
    "- **Graph Neural Networks**: GNN applications in biology\n",
    "\n",
    "### ü§ù **Community Support**\n",
    "- **GitHub Issues**: Report bugs and request features\n",
    "- **Discussions**: Join community discussions\n",
    "- **Contributions**: Contribute to the project development\n",
    "\n",
    "## üéØ Final Reminders\n",
    "\n",
    "### ‚úÖ **Success Checklist**\n",
    "- [ ] Data processed successfully\n",
    "- [ ] PyTorch Geometric format validated\n",
    "- [ ] Graph structure verified\n",
    "- [ ] File saved and tested\n",
    "- [ ] Ready for model training\n",
    "\n",
    "### üí° **Key Takeaways**\n",
    "1. **Data Quality**: High-quality input data is crucial for good results\n",
    "2. **Spatial Context**: Graph structure preserves tissue architecture\n",
    "3. **Standardization**: Consistent preprocessing ensures reproducibility\n",
    "4. **Validation**: Always validate processed data before training\n",
    "5. **Documentation**: Keep detailed records of processing steps\n",
    "\n",
    "## üöÄ **You're Ready to Go!**\n",
    "\n",
    "Your spatial transcriptomics data is now fully prepared for Hist2Cell analysis. The processed data contains all the necessary components for training, inference, and visualization. \n",
    "\n",
    "**Next up**: Head to the training tutorial to learn how to train your Hist2Cell model!\n",
    "\n",
    "---\n",
    "\n",
    "*Happy analyzing! üß¨‚ú®*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hist2Cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
