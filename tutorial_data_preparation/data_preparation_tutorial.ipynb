{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the subgraph sampling training in `Hist2Cell`, we need to prepare the dataset into certain data structure.\n",
    "\n",
    "In `./example_data/humanlung_cell2location` and `./example_data/humanlung_cell2location_2x`, we provide the processed data for the humanlung cell2location dataset in our study.\n",
    "In this tutorial, we will go through the data structure of the provided processed data.\n",
    "\n",
    "Then, we will show how to prepare your own raw data into this data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "First, let's see the data structure of process data for slide `WSA_LngSP9258467` from donor A50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[422, 3, 224, 224], edge_index=[2, 2732], y=[422, 330], pos=[422, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "processed_data = torch.load(\"../example_data/humanlung_cell2location/WSA_LngSP9258467.pt\")\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the processed  data, the following varibales are:\n",
    "- `x`: the 224*224*3 image patch for each spot in the ST data, each spot is considered as one node in the graphs representing the slide, there are in total 422 spots in this slide;\n",
    "- `edge_index`: graph connectivity in `COO` format with shape `[2, num_edges]`, there are 2732 edges in this slide;\n",
    "- `y`: the label for each spot, contains 250 highly expressed gene labels + 80 fine-grained cell abundance labels, resulting 330 labels for each spot;\n",
    "- `pos`: the x-y pixel coordinate of each spot on the original slide, used in visualization and calculating cell-colocalization metric;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['x'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3157, 2.3157, 2.3157, 4.5253, 3.3463])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['y'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ..., 421, 421, 421],\n",
       "        [  0,  54,  96,  ..., 250, 321, 421]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['edge_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12701.,  9136.],\n",
       "        [15552., 10662.],\n",
       "        [12372.,  9708.],\n",
       "        [15332., 10662.],\n",
       "        [15003.,  7801.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['pos'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `DataLoader`, we use `NeighborLoader` from `torch_geometric` , which supports subgraph sampling from some center nodes, here are 2 important parameters:\n",
    "- `hop`: this parameter define receptive field when sampling the subgraphs with a group of center nodes for training/testing, in our paper, we use 2-hop subgraphs to achieve a banlance between computation cost and performance, generally, bigger receptive field will contain more neighboring information.\n",
    "- `subgraph_bs`: this parameter define the number of subgraphs to be sampled during training/testing, which is the `subgraph batchsize`, we use `subgraph_bs=16` on our RTX 3090 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wqzhao/anaconda3/envs/Hist2Cell/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:55: UserWarning: The usage of the 'directed' argument in 'NeighborSampler' is deprecated. Use `subgraph_type='induced'` instead.\n",
      "  warnings.warn(f\"The usage of the 'directed' argument in \"\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "import torch_geometric\n",
    "torch_geometric.typing.WITH_PYG_LIB = False\n",
    "\n",
    "\n",
    "hop = 2\n",
    "subgraph_bs = 16\n",
    "\n",
    "dataloader_loader = NeighborLoader(\n",
    "    processed_data,\n",
    "    num_neighbors=[-1]*hop,\n",
    "    batch_size=subgraph_bs,\n",
    "    directed=False,\n",
    "    input_nodes=None,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch_geometric`, the sampled subgraphs are merged into a big graph for parallel training, for more details, please refer to the documentation of torch_geometric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[202, 3, 224, 224], edge_index=[2, 1140], y=[202, 330], pos=[202, 2], n_id=[202], e_id=[1140], input_id=[16], batch_size=16)\n"
     ]
    }
   ],
   "source": [
    "for subgraphs in dataloader_loader:\n",
    "    print(subgraphs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show how to preprocess raw data into this structure for `Hist2Cell` training:\n",
    "\n",
    "We upload the raw data of slide `WSA_LngSP9258467` in `./example_data/example_raw_data/WSA_LngSP9258467` for preprocessing tutorial.\n",
    "\n",
    "We explain the files under this folder one by one:\n",
    "- `patch`: folder contain the image patches for each spot in the ST data;\n",
    "- `cell_ratio.csv`: contains the 80 fine-grained cell type abundances;\n",
    "- `stdata.csv`: contains the spatial gene expression for each spot;\n",
    "- `log1p_stdata.csv`: contains the log1p processed spatial gene expression for each spot;\n",
    "- `spots.csv`: contains the pixel coordinate for each spot;\n",
    "- `high_250_stdata.csv`: contains the top 250 highly expressed spatial gene expression for each spot;\n",
    "- `high_250_stdata_log1p.csv`: contains the log1p processed top 250 highly expressed spatial gene expression for each spot;\n",
    "- `WSA_LngSP9258467.jpg`: the original slide image;\n",
    "- `WSA_LngSP9258467_low_res.jpg`: the low-resolution slide image, for quick visualization and other processing;\n",
    "- `spot_view.jpg`: the original slide image with visualized spots;\n",
    "- `2x_patch`: folder contain the image patches for each spot in 2x super-resolved experiments;\n",
    "- `2x_spots.csv`: contains the pixel coordinate for each image patch in the 2x super-resolved experiments;\n",
    "- `2x_spot_view.jpg`: the original slide image with visualized 2x resolution spots;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the raw data, we first define a `STDataset` to iterate the spots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class STDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, slide,transform=None):\n",
    "        super(STDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.slide = slide\n",
    "        self.transform = transform\n",
    "\n",
    "        patch_path = os.path.join(root, slide, 'patches')\n",
    "        patch = os.listdir(patch_path)\n",
    "        patch_list = [x.split('.')[0] for x in patch]\n",
    "\n",
    "        cell_label = pd.read_csv(os.path.join(root, slide, 'cell_ratio.csv'), index_col=0)\n",
    "        gene_label = pd.read_csv(os.path.join(root, slide, 'high_250_stdata.csv'), index_col=0)\n",
    "        label_df = pd.merge(gene_label, cell_label, left_index=True, right_index=True)\n",
    "\n",
    "        label_index_set = set(label_df.index)\n",
    "        patch_index_set = set(patch_list)\n",
    "        and_set = label_index_set & patch_index_set\n",
    "\n",
    "        patch_list = list(and_set)\n",
    "        self.label_df = label_df.loc[patch_list]\n",
    "        self.patch = patch_list\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        patch_id = self.patch[index]\n",
    "        patch_path = os.path.join(self.root, self.slide, 'patches', patch_id)\n",
    "        patch = Image.open(patch_path+'.jpg').convert('RGB')\n",
    "        data = transforms.Resize((224, 224))(patch)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        label = self.label_df.loc[patch_id].values\n",
    "        label = torch.Tensor(label)\n",
    "\n",
    "        return patch_id, data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform_pcam = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_data = STDataset(root=\"../example_data/example_raw_data\", slide=\"WSA_LngSP9258467\",transform=test_transform_pcam)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=512, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate this `STDataset`, save the spot image patch, the spot labels, and the spot id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_data_array = []\n",
    "spot_label_array = []\n",
    "spot_id_array = []\n",
    "\n",
    "for name, data, label in test_loader:\n",
    "    spot_id_array.append(list(name))\n",
    "    label = label.float()\n",
    "    label = label.squeeze()\n",
    "    spot_label_array.append(label.detach().numpy())\n",
    "    spot_data_array.append(data.detach().numpy())\n",
    "\n",
    "for i in range(len(spot_data_array)):\n",
    "    if len(spot_data_array[i].shape) <= 1:\n",
    "        spot_data_array[i] = spot_data_array[i][np.newaxis, :]\n",
    "for i in range(len(spot_label_array)):\n",
    "    if len(spot_label_array[i].shape) <= 1:\n",
    "        spot_label_array[i] = spot_label_array[i][np.newaxis, :]\n",
    "        \n",
    "spot_data_array = np.concatenate(spot_data_array)\n",
    "spot_label_array = np.concatenate(spot_label_array)\n",
    "spot_ids = list()\n",
    "for ids in spot_id_array:\n",
    "    spot_ids=spot_ids+ids\n",
    "spot_id_array = spot_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422, 3, 224, 224)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422, 3, 224, 224)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WSA_LngSP9258467_AACTTGCCCGTATGCA-1',\n",
       " 'WSA_LngSP9258467_CCCAATTTCACAACTT-1',\n",
       " 'WSA_LngSP9258467_CGCAATTCTACAATAA-1',\n",
       " 'WSA_LngSP9258467_GTCTGGGCGGTCGAGA-1',\n",
       " 'WSA_LngSP9258467_AAATGGTCAATGTGCC-1']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_id_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the graph, we need to have the `array_col` and `array_row` for each spot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "\n",
    "adata = sc.read(\"../example_data/example_raw_data/sp.X_norm5e4_log1p.h5ad\")\n",
    "spot_array_cols = adata.obs.array_col\n",
    "spot_array_rows = adata.obs.array_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spot_id\n",
       "WSA_LngSP8759311_AAACAAGTATCTCCCA-1    102\n",
       "WSA_LngSP8759311_AAACAGAGCGACTCCT-1     94\n",
       "WSA_LngSP8759311_AAACATTTCCCGGATT-1     97\n",
       "WSA_LngSP8759311_AAACCCGAACGAAATC-1    115\n",
       "WSA_LngSP8759311_AAACCGTTCGTCCAGG-1     42\n",
       "Name: array_col, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_array_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `./example_data/example_raw_data/WSA_LngSP9258467/spot_view.jpg`, we can see that every spot has 6 nearest neighbors, build the graph according to this spatial relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 29], [33, 39], [28, 26], [44, 46], [51, 33]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_array_x_y = []\n",
    "for item in spot_id_array:\n",
    "    spot_array_x_y.append([int(spot_array_cols[item]), int(spot_array_rows[item])])\n",
    "    \n",
    "spot_array_x_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = np.zeros((len(spot_array_x_y), len(spot_array_x_y)))\n",
    "\n",
    "for i in range(len(spot_array_x_y)):\n",
    "    for j in range(len(spot_array_x_y)):\n",
    "        if i == j:\n",
    "            adj[i][j] = 1.0\n",
    "        else:\n",
    "            x1 = spot_array_x_y[i][0]\n",
    "            y1 = spot_array_x_y[i][1]\n",
    "            x2 = spot_array_x_y[j][0]\n",
    "            y2 = spot_array_x_y[j][1]\n",
    "\n",
    "            if x2 <= x1 - 3 or x2 >= x1 + 3 or y2 <= y1 - 2 or y2 >= y1 + 2:\n",
    "                continue\n",
    "            else:\n",
    "                adj[i][j] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy visualization, we also save the pixel coordinate of each spot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12263,  7610],\n",
       "       [12920,  9517],\n",
       "       [12372,  7037],\n",
       "       [14126, 10853],\n",
       "       [14894,  8373]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spots_coord = pd.read_csv(os.path.join(\"../example_data/example_raw_data/WSA_LngSP9258467/spots.csv\"), index_col=0)\n",
    "spots_coord = spots_coord.loc[spot_id_array].values\n",
    "spots_coord[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the processed data as `torch_geometric.data.Data` for `Hist2Cell` trainng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[422, 3, 224, 224], edge_index=[2, 2732], y=[422, 330], pos=[422, 2], spot_id=[422])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Data\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "x = Tensor(spot_data_array)\n",
    "y = Tensor(spot_label_array)\n",
    "adj = Tensor(adj)\n",
    "edge_index, _ = dense_to_sparse(adj)\n",
    "pos = Tensor(spots_coord)\n",
    "data = Data(x=x, edge_index=edge_index, y=y, pos=pos, spot_id=spot_id_array)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, \"../example_data/example_processed_data/WSA_LngSP9258467.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
